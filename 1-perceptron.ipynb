{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of the practical work\n",
    "\n",
    "The objective is to get hands on experience on the fundamental elements of neural networks:\n",
    " \n",
    " - perceptron architecture (linear regression)\n",
    " - loss function\n",
    " - empirical loss\n",
    " - gradient descent\n",
    "\n",
    "For this we will implement from scratch the data-structure and algorithms to train a perceptron. Note that slides related to the perceptron and neural networks in general are available on [moodle](https://moodle.insa-toulouse.fr/course/view.php?id=1790).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The objective of the regression is the prediction of the hydrodynamic performance of sailing yachts from dimensions and velocity.\n",
    "The **inputs** are linked to dimension and hydrodynamics characteristics:\n",
    "1. Longitudinal position of the center of buoyancy\n",
    "(*flottabilité*), adimensional.\n",
    "2. Prismatic coefficient, adimensional.\n",
    "3. Length-displacement ratio, adimensional.\n",
    "4. Beam -draught ratio ((*tiran d’eau*), adimensional.\n",
    "5. Length-beam ratio, adimensional.\n",
    "6. Froude number, adimensional\n",
    "\n",
    "**Target value/predicted value (Output)** = Residuary resistance per unit weight of\n",
    "displacement, adimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful libraries and functions\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "def print_stats(dataset):\n",
    "    \"\"\"Print statistics of a dataset\"\"\"\n",
    "    print(pandas.DataFrame(dataset).describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset available\n"
     ]
    }
   ],
   "source": [
    "# Download the data set and place in the current folder (works on linux only)\n",
    "filename = 'yacht_hydrodynamics.data'\n",
    "\n",
    "import os.path\n",
    "import requests\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    print(\"Downloading dataset...\")\n",
    "    r = requests.get('https://arbimo.github.io/tp-supervised-learning/tp1/' + filename)\n",
    "    open(filename , 'wb').write(r.content)\n",
    "    \n",
    "print('Dataset available')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset\n",
    "\n",
    "- how many examples are there in the dataset?\n",
    "- how many features for each example?\n",
    "- what is the ground truth of the 10th example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 308\n",
      "Number of features: 6\n",
      "Ground truth of the 10th example: 1.83\n",
      "\n",
      " f([-5.    0.6   4.78  4.24  3.15  0.35]) = 8.62\n",
      "\n",
      " f([-5.     0.565  4.77   3.99   3.15   0.15 ]) = 0.18\n",
      "\n",
      " f([-2.3    0.565  4.78   5.35   2.76   0.15 ]) = 0.29\n",
      "\n",
      " f([-5.     0.6    4.78   4.24   3.15   0.325]) = 6.2\n",
      "\n",
      " f([0.    0.53  4.78  3.75  3.15  0.175]) = 0.59\n"
     ]
    }
   ],
   "source": [
    "# load the dataset and slip between inputs (X) and ground truth (Y)\n",
    "dataset = np.genfromtxt(\"yacht_hydrodynamics.data\", delimiter='')\n",
    "\n",
    "# How many examples are there in the dataset ?\n",
    "print(f\"Number of examples: {dataset.shape[0]}\")\n",
    "# > Number of examples: 308\n",
    "\n",
    "# How many features for each example ?\n",
    "print(f\"Number of features: {dataset.shape[1]-1}\")\n",
    "# > Number of features: 6\n",
    "\n",
    "# What s the ground truth of the 10th example ?\n",
    "print(f\"Ground truth of the 10th example: {dataset[9, -1]}\")\n",
    "# Ground truth of the 10th example: 1.83\n",
    "\n",
    "X = dataset[:, :-1] # examples features -  all rows with all elements in rows except last one\n",
    "Y = dataset[:, -1]  # ground truth - last element in all rows\n",
    "\n",
    "# Print the first 5 examples\n",
    "for i in range(0,5):\n",
    "    print(f\"\\n f({X[i]}) = {Y[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command adds a column to the inputs.\n",
    "\n",
    "- what is in the value added this column?\n",
    "- why are we doing this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0           1           2           3           4           5  \\\n",
      "count  308.0  308.000000  308.000000  308.000000  308.000000  308.000000   \n",
      "mean     1.0   -2.381818    0.564136    4.788636    3.936818    3.206818   \n",
      "std      0.0    1.513219    0.023290    0.253057    0.548193    0.247998   \n",
      "min      1.0   -5.000000    0.530000    4.340000    2.810000    2.730000   \n",
      "25%      1.0   -2.400000    0.546000    4.770000    3.750000    3.150000   \n",
      "50%      1.0   -2.300000    0.565000    4.780000    3.955000    3.150000   \n",
      "75%      1.0   -2.300000    0.574000    5.100000    4.170000    3.510000   \n",
      "max      1.0    0.000000    0.600000    5.140000    5.350000    3.640000   \n",
      "\n",
      "                6  \n",
      "count  308.000000  \n",
      "mean     0.287500  \n",
      "std      0.100942  \n",
      "min      0.125000  \n",
      "25%      0.200000  \n",
      "50%      0.287500  \n",
      "75%      0.375000  \n",
      "max      0.450000  \n"
     ]
    }
   ],
   "source": [
    "X = np.insert(X, 0, np.ones((len(X))), axis= 1)\n",
    "print_stats(X)\n",
    "\n",
    "# What is in the value added this column ? X = np.insert(X, 0, np.ones((len(X))), axis= 1)\n",
    "\n",
    "# > The value added in this column is only one, we insert it at the beginning because it is the constant vector.\n",
    "\n",
    "# Why are we doing this ?\n",
    "\n",
    "# > Because we want to add a constant vector to the matrix X, so that we can multiply it by the vector of weights w, and thus obtain the vector of predictions y.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the perceptron\n",
    "\n",
    "![Perceptron for regression](https://arbimo.github.io/tp-supervised-learning/2223-ae/tp1/perceptron-regression.png)\n",
    "\n",
    "We now want to define a perceptron, that is, a function of the form: \n",
    "\n",
    "$h_w(x) = w_0 + w_1 \\times x_1 + \\dots + w_n \\times x_n$\n",
    "\n",
    "- Complete the code snippet below to:\n",
    "  - create the vector of weight `w`, initialize to arbitrary values (we suggest 0)\n",
    "  - implement the `h` function that evaluate an example based on the vector of weights\n",
    "  - check if this works on a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " h([ 1.   -5.    0.6   4.78  4.24  3.15  0.35]) = 0.0\n",
      "\n",
      " h([ 1.    -5.     0.565  4.77   3.99   3.15   0.15 ]) = 0.0\n",
      "\n",
      " h([ 1.    -2.3    0.565  4.78   5.35   2.76   0.15 ]) = 0.0\n",
      "\n",
      " h([ 1.    -5.     0.6    4.78   4.24   3.15   0.325]) = 0.0\n",
      "\n",
      " h([1.    0.    0.53  4.78  3.75  3.15  0.175]) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Create the vector of weight w, initialize to arbitrary values (e.g. 0)\n",
    "w = np.zeros((len(X[0])))\n",
    "\n",
    "# Implement h function that evaluate an exampled based on the vector of weights\n",
    "def h(w, x):\n",
    "    return w@x\n",
    "\n",
    "# Check if this works on a few examples\n",
    "for i in range(0,5):\n",
    "    print(f\"\\n h({X[i]}) = {h(w, X[i])}\")\n",
    "\n",
    "# print the ground truth and the evaluation of h_w on the first example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Complete the definiton of the loss function below such that, for a **single** example `x` with ground truth `y`, it returns the $L_2$ loss of $h_w$ on `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the loss function s.t for a single example x with ground truth y, it returns the L2 loss of h_w on x\n",
    "def loss(w, x, y):\n",
    "    return (h(w, x) - y)**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical loss\n",
    "\n",
    "Complete the function below to compute the empirical loss of $h_w$ on a **set** of examples $X$ with associated ground truths $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the empirical loss of h_w on a set of examples X with ground truth Y\n",
    "def emp_loss(w, X, Y):\n",
    "    cnt = 0\n",
    "    for i in range(len(X)):\n",
    "        cnt = cnt + loss(w, X[i], Y[i])\n",
    "    return cnt/len(X)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient update\n",
    "\n",
    "A gradient update is of the form: $w \\gets w + dw$\n",
    "\n",
    "- Complete the function below so that it computes the $dw$ term (the 'update') based on a set of examples `(X, Y)` the step (`alpha`)\n",
    "\n",
    "(you can look at slide 32 of the ANN lecture slides for an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.09907143e-05, -4.91131299e-05,  1.18215159e-05,  1.00494203e-04,\n",
       "        8.24308351e-05,  6.73057182e-05,  8.50620292e-06])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete s.t it computes the dw term (\"update\") based on a set of examples (X, Y) and the step alpha\n",
    "def compute_update(w, X, Y, alpha):\n",
    "    dw = np.zeros((len(X[0])))\n",
    "    for i in range(len(X)):\n",
    "        dw += (h(w, X[i]) - Y[i])*2*X[i]\n",
    "    return - alpha*dw / len(X)\n",
    "\n",
    "compute_update(w, X, Y, alpha = 10e-7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Now implement the gradient descent algorithm that will:\n",
    "\n",
    "- repeatedly apply an update the weights \n",
    "- stops when a max number of iterations is reached (do not consider early stopping for now)\n",
    "- returns the final vector of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(w_init, X, Y, alpha, max_iter):\n",
    "    w = w_init\n",
    "    for i in range(max_iter):\n",
    "        w += compute_update(w, X, Y, alpha)\n",
    "    return w    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploitation\n",
    "\n",
    "You gradient descent is now complete and you can exploit it to train your perceptron.\n",
    "\n",
    "- Train your perceptron to get a model.\n",
    "- Visualize the evolution of the loss on the training set. Has it converged?\n",
    "- Try training for several choices of `alpha` and `max_iter`. What seem like a reasonable choice?\n",
    "- What is the loss associated with the final model?\n",
    "- Is the final model the optimal one for a perceptron?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "316.1330608186365\n",
      "1\n",
      "283.5657137640746\n",
      "2\n",
      "256.31174740632713\n",
      "3\n",
      "240.2460211667162\n",
      "4\n",
      "233.19025268710783\n",
      "5\n",
      "230.7958607635228\n",
      "6\n",
      "230.11853705261598\n",
      "7\n",
      "229.90656151811993\n",
      "8\n",
      "229.78508409288415\n",
      "9\n",
      "229.6718324893739\n",
      "10\n",
      "229.5537676992513\n",
      "11\n",
      "229.42990897214486\n",
      "12\n",
      "229.30079768154366\n",
      "13\n",
      "229.16704444516213\n",
      "14\n",
      "229.02917077263942\n",
      "15\n",
      "228.8875928348542\n",
      "16\n",
      "228.74262096086494\n",
      "17\n",
      "228.59446390935207\n",
      "18\n",
      "228.44323655457856\n",
      "19\n",
      "228.28897015909288\n",
      "20\n",
      "228.13162440486494\n",
      "21\n",
      "227.9711003790562\n",
      "22\n",
      "227.8072537792628\n",
      "23\n",
      "227.63990770559275\n",
      "24\n",
      "227.46886453117386\n",
      "25\n",
      "227.29391647678216\n",
      "26\n",
      "227.1148546485769\n",
      "27\n",
      "226.93147642166826\n",
      "28\n",
      "226.74359115993857\n",
      "29\n",
      "226.5510243500905\n",
      "30\n",
      "226.3536202934638\n",
      "31\n",
      "226.15124354289017\n",
      "32\n",
      "225.9437792954216\n",
      "33\n",
      "225.73113295798504\n",
      "34\n",
      "225.51322909532368\n",
      "35\n",
      "225.29000995163256\n",
      "36\n",
      "225.0614337126708\n",
      "37\n",
      "224.82747264698628\n",
      "38\n",
      "224.58811123591371\n",
      "39\n",
      "224.3433443742393\n",
      "40\n",
      "224.09317569832987\n",
      "41\n",
      "223.83761607694953\n",
      "42\n",
      "223.57668228237384\n",
      "43\n",
      "223.31039584572977\n",
      "44\n",
      "223.03878209050143\n",
      "45\n",
      "222.76186933139908\n",
      "46\n",
      "222.47968822174593\n",
      "47\n",
      "222.19227123063712\n",
      "48\n",
      "221.89965223082962\n",
      "49\n",
      "221.601866179151\n",
      "50\n",
      "221.29894887274165\n",
      "51\n",
      "220.99093676639063\n",
      "52\n",
      "220.67786683829044\n",
      "53\n",
      "220.35977649358594\n",
      "54\n",
      "220.03670349699976\n",
      "55\n",
      "219.70868592751717\n",
      "56\n",
      "219.37576214957159\n",
      "57\n",
      "219.03797079641237\n",
      "58\n",
      "218.69535076231963\n",
      "59\n",
      "218.3479412011499\n",
      "60\n",
      "217.99578152931568\n",
      "61\n",
      "217.6389114317872\n",
      "62\n",
      "217.27737087007623\n",
      "63\n",
      "216.91120009143205\n",
      "64\n",
      "216.54043963868946\n",
      "65\n",
      "216.1651303603504\n",
      "66\n",
      "215.78531342059594\n",
      "67\n",
      "215.40103030900175\n",
      "68\n",
      "215.01232284978414\n",
      "69\n",
      "214.61923321044986\n",
      "70\n",
      "214.22180390975487\n",
      "71\n",
      "213.82007782488995\n",
      "72\n",
      "213.4140981978479\n",
      "73\n",
      "213.00390864091918\n",
      "74\n",
      "212.5895531412938\n",
      "75\n",
      "212.17107606474613\n",
      "76\n",
      "211.7485221583909\n",
      "77\n",
      "211.3219365525077\n",
      "78\n",
      "210.89136476143543\n",
      "79\n",
      "210.45685268354774\n",
      "80\n",
      "210.01844660032117\n",
      "81\n",
      "209.57619317451875\n",
      "82\n",
      "209.13013944750807\n",
      "83\n",
      "208.6803328357492\n",
      "84\n",
      "208.22682112647166\n",
      "85\n",
      "207.769652472589\n",
      "86\n",
      "207.30887538687213\n",
      "87\n",
      "206.84453873543336\n",
      "88\n",
      "206.37669173054883\n",
      "89\n",
      "205.90538392287002\n",
      "90\n",
      "205.4306651930603\n",
      "91\n",
      "204.95258574289898\n",
      "92\n",
      "204.47119608589904\n",
      "93\n",
      "203.98654703747292\n",
      "94\n",
      "203.4986897046975\n",
      "95\n",
      "203.00767547570948\n",
      "96\n",
      "202.51355600877721\n",
      "97\n",
      "202.01638322108326\n",
      "98\n",
      "201.51620927725764\n",
      "99\n",
      "201.01308657769766\n",
      "100\n",
      "200.50706774670468\n",
      "101\n",
      "199.99820562047293\n",
      "102\n",
      "199.48655323496206\n",
      "103\n",
      "198.97216381367875\n",
      "104\n",
      "198.45509075539863\n",
      "105\n",
      "197.93538762184832\n",
      "106\n",
      "197.41310812537583\n",
      "107\n",
      "196.88830611662814\n",
      "108\n",
      "196.36103557225587\n",
      "109\n",
      "195.83135058266313\n",
      "110\n",
      "195.29930533982034\n",
      "111\n",
      "194.76495412515018\n",
      "112\n",
      "194.22835129750587\n",
      "113\n",
      "193.68955128124858\n",
      "114\n",
      "193.14860855443536\n",
      "115\n",
      "192.60557763712796\n",
      "116\n",
      "192.06051307982804\n",
      "117\n",
      "191.51346945204563\n",
      "118\n",
      "190.96450133100936\n",
      "119\n",
      "190.41366329051792\n",
      "120\n",
      "189.8610098899379\n",
      "121\n",
      "189.3065956633556\n",
      "122\n",
      "188.7504751088758\n",
      "123\n",
      "188.19270267807718\n",
      "124\n",
      "187.63333276562142\n",
      "125\n",
      "187.0724196990139\n",
      "126\n",
      "186.5100177285234\n",
      "127\n",
      "185.94618101724973\n",
      "128\n",
      "185.3809636313487\n",
      "129\n",
      "184.8144195304069\n",
      "130\n",
      "184.24660255796505\n",
      "131\n",
      "183.67756643219337\n",
      "132\n",
      "183.10736473671173\n",
      "133\n",
      "182.53605091155447\n",
      "134\n",
      "181.9636782442802\n",
      "135\n",
      "181.3902998612228\n",
      "136\n",
      "180.81596871888084\n",
      "137\n",
      "180.24073759544606\n",
      "138\n",
      "179.66465908246624\n",
      "139\n",
      "179.087785576644\n",
      "140\n",
      "178.5101692717665\n",
      "141\n",
      "177.93186215076497\n",
      "142\n",
      "177.35291597790646\n",
      "143\n",
      "176.77338229110978\n",
      "144\n",
      "176.19331239438947\n",
      "145\n",
      "175.61275735042364\n",
      "146\n",
      "175.03176797324525\n",
      "147\n",
      "174.45039482105804\n",
      "148\n",
      "173.86868818917065\n",
      "149\n",
      "173.28669810305337\n",
      "150\n",
      "172.7044743115136\n",
      "151\n",
      "172.12206627999117\n",
      "152\n",
      "171.53952318397094\n",
      "153\n",
      "170.95689390251366\n",
      "154\n",
      "170.37422701190283\n",
      "155\n",
      "169.79157077941025\n",
      "156\n",
      "169.20897315717625\n",
      "157\n",
      "168.6264817762064\n",
      "158\n",
      "168.04414394048368\n",
      "159\n",
      "167.46200662119722\n",
      "160\n",
      "166.88011645108472\n",
      "161\n",
      "166.2985197188915\n",
      "162\n",
      "165.71726236394426\n",
      "163\n",
      "165.1363899708401\n",
      "164\n",
      "164.55594776424914\n",
      "165\n",
      "163.97598060383464\n",
      "166\n",
      "163.39653297928632\n",
      "167\n",
      "162.81764900546955\n",
      "168\n",
      "162.23937241768837\n",
      "169\n",
      "161.6617465670654\n",
      "170\n",
      "161.08481441603593\n",
      "171\n",
      "160.50861853395645\n",
      "172\n",
      "159.93320109282968\n",
      "173\n",
      "159.35860386314414\n",
      "174\n",
      "158.78486820982786\n",
      "175\n",
      "158.2120350883201\n",
      "176\n",
      "157.6401450407546\n",
      "177\n",
      "157.06923819226085\n",
      "178\n",
      "156.49935424737822\n",
      "179\n",
      "155.93053248658808\n",
      "180\n",
      "155.36281176295654\n",
      "181\n",
      "154.79623049889602\n",
      "182\n",
      "154.23082668303888\n",
      "183\n",
      "153.6666378672268\n",
      "184\n",
      "153.10370116361514\n",
      "185\n",
      "152.54205324188794\n",
      "186\n",
      "151.98173032659125\n",
      "187\n",
      "151.42276819457803\n",
      "188\n",
      "150.86520217256538\n",
      "189\n",
      "150.30906713480675\n",
      "190\n",
      "149.75439750087605\n",
      "191\n",
      "149.2012272335634\n",
      "192\n",
      "148.649589836884\n",
      "193\n",
      "148.09951835419804\n",
      "194\n",
      "147.55104536644112\n",
      "195\n",
      "147.00420299046695\n",
      "196\n",
      "146.45902287749772\n",
      "197\n",
      "145.91553621168677\n",
      "198\n",
      "145.37377370878858\n",
      "199\n",
      "144.83376561493705\n",
      "200\n",
      "144.29554170553345\n",
      "201\n",
      "143.75913128423957\n",
      "202\n",
      "143.22456318207918\n",
      "203\n",
      "142.6918657566445\n",
      "204\n",
      "142.16106689140773\n",
      "205\n",
      "141.63219399513804\n",
      "206\n",
      "141.10527400142152\n",
      "207\n",
      "140.58033336828387\n",
      "208\n",
      "140.0573980779174\n",
      "209\n",
      "139.53649363650564\n",
      "210\n",
      "139.01764507415305\n",
      "211\n",
      "138.5008769449102\n",
      "212\n",
      "137.9862133269004\n",
      "213\n",
      "137.4736778225439\n",
      "214\n",
      "136.96329355887787\n",
      "215\n",
      "136.45508318797332\n",
      "216\n",
      "135.94906888744796\n",
      "217\n",
      "135.4452723610707\n",
      "218\n",
      "134.9437148394616\n",
      "219\n",
      "134.44441708088289\n",
      "220\n",
      "133.9473993721208\n",
      "221\n",
      "133.4526815294585\n",
      "222\n",
      "132.96028289973577\n",
      "223\n",
      "132.47022236149945\n",
      "224\n",
      "131.98251832623816\n",
      "225\n",
      "131.49718873970366\n",
      "226\n",
      "131.01425108331597\n",
      "227\n",
      "130.5337223756545\n",
      "228\n",
      "130.05561917402653\n",
      "229\n",
      "129.57995757612102\n",
      "230\n",
      "129.10675322174035\n",
      "231\n",
      "128.63602129461074\n",
      "232\n",
      "128.16777652427055\n",
      "233\n",
      "127.70203318803472\n",
      "234\n",
      "127.2388051130336\n",
      "235\n",
      "126.7781056783255\n",
      "236\n",
      "126.31994781708364\n",
      "237\n",
      "125.86434401885094\n",
      "238\n",
      "125.41130633186825\n",
      "239\n",
      "124.96084636546847\n",
      "240\n",
      "124.51297529253965\n",
      "241\n",
      "124.06770385205424\n",
      "242\n",
      "123.62504235166213\n",
      "243\n",
      "123.18500067034785\n",
      "244\n",
      "122.74758826115063\n",
      "245\n",
      "122.31281415394334\n",
      "246\n",
      "121.88068695827396\n",
      "247\n",
      "121.45121486626226\n",
      "248\n",
      "121.02440565555624\n",
      "249\n",
      "120.6002666923415\n",
      "250\n",
      "120.1788049344074\n",
      "251\n",
      "119.76002693426406\n",
      "252\n",
      "119.34393884231281\n",
      "253\n",
      "118.93054641006562\n",
      "254\n",
      "118.51985499341355\n",
      "255\n",
      "118.11186955594448\n",
      "256\n",
      "117.70659467230486\n",
      "257\n",
      "117.30403453160771\n",
      "258\n",
      "116.90419294088437\n",
      "259\n",
      "116.50707332857789\n",
      "260\n",
      "116.11267874807773\n",
      "261\n",
      "115.72101188129433\n",
      "262\n",
      "115.33207504227168\n",
      "263\n",
      "114.94587018083689\n",
      "264\n",
      "114.56239888628589\n",
      "265\n",
      "114.18166239110263\n",
      "266\n",
      "113.80366157471272\n",
      "267\n",
      "113.42839696726732\n",
      "268\n",
      "113.05586875345877\n",
      "269\n",
      "112.68607677636415\n",
      "270\n",
      "112.31902054131844\n",
      "271\n",
      "111.95469921981253\n",
      "272\n",
      "111.59311165341751\n",
      "273\n",
      "111.23425635773306\n",
      "274\n",
      "110.8781315263582\n",
      "275\n",
      "110.52473503488424\n",
      "276\n",
      "110.17406444490709\n",
      "277\n",
      "109.82611700806008\n",
      "278\n",
      "109.48088967006301\n",
      "279\n",
      "109.13837907478889\n",
      "280\n",
      "108.79858156834634\n",
      "281\n",
      "108.461493203175\n",
      "282\n",
      "108.12710974215489\n",
      "283\n",
      "107.79542666272859\n",
      "284\n",
      "107.46643916103088\n",
      "285\n",
      "107.14014215603254\n",
      "286\n",
      "106.81653029368766\n",
      "287\n",
      "106.49559795109221\n",
      "288\n",
      "106.17733924064693\n",
      "289\n",
      "105.86174801422447\n",
      "290\n",
      "105.5488178673431\n",
      "291\n",
      "105.23854214333973\n",
      "292\n",
      "104.93091393754769\n",
      "293\n",
      "104.62592610147423\n",
      "294\n",
      "104.3235712469767\n",
      "295\n",
      "104.02384175043831\n",
      "296\n",
      "103.72672975694148\n",
      "297\n",
      "103.43222718443738\n",
      "298\n",
      "103.14032572791177\n",
      "299\n",
      "102.85101686354334\n",
      "300\n",
      "102.56429185285916\n",
      "301\n",
      "102.28014174687915\n",
      "302\n",
      "101.99855739025486\n",
      "303\n",
      "101.7195294253979\n",
      "304\n",
      "101.44304829659788\n",
      "305\n",
      "101.16910425413012\n",
      "306\n",
      "100.89768735835183\n",
      "307\n",
      "100.62878748378343\n",
      "308\n",
      "100.36239432317934\n",
      "309\n",
      "100.0984973915811\n",
      "310\n",
      "99.83708603035744\n",
      "311\n",
      "99.57814941122719\n",
      "312\n",
      "99.32167654026551\n",
      "313\n",
      "99.06765626189264\n",
      "314\n",
      "98.81607726284356\n",
      "315\n",
      "98.56692807611947\n",
      "316\n",
      "98.32019708491795\n",
      "317\n",
      "98.07587252654373\n",
      "318\n",
      "97.83394249629744\n",
      "319\n",
      "97.59439495134244\n",
      "320\n",
      "97.35721771454872\n",
      "321\n",
      "97.12239847831367\n",
      "322\n",
      "96.88992480835897\n",
      "323\n",
      "96.65978414750248\n",
      "324\n",
      "96.43196381940432\n",
      "325\n",
      "96.20645103228901\n",
      "326\n",
      "95.98323288263889\n",
      "327\n",
      "95.76229635886232\n",
      "328\n",
      "95.54362834493215\n",
      "329\n",
      "95.32721562399912\n",
      "330\n",
      "95.11304488197455\n",
      "331\n",
      "94.9011027110838\n",
      "332\n",
      "94.69137561339161\n",
      "333\n",
      "94.4838500042959\n",
      "334\n",
      "94.27851221599167\n",
      "335\n",
      "94.07534850090452\n",
      "336\n",
      "93.87434503509195\n",
      "337\n",
      "93.67548792161257\n",
      "338\n",
      "93.4787631938648\n",
      "339\n",
      "93.2841568188907\n",
      "340\n",
      "93.09165470064877\n",
      "341\n",
      "92.9012426832528\n",
      "342\n",
      "92.71290655417594\n",
      "343\n",
      "92.52663204742345\n",
      "344\n",
      "92.34240484666847\n",
      "345\n",
      "92.1602105883553\n",
      "346\n",
      "91.9800348647662\n",
      "347\n",
      "91.80186322705404\n",
      "348\n",
      "91.62568118824\n",
      "349\n",
      "91.45147422617441\n",
      "350\n",
      "91.27922778646315\n",
      "351\n",
      "91.10892728535688\n",
      "352\n",
      "90.9405581126048\n",
      "353\n",
      "90.77410563427193\n",
      "354\n",
      "90.60955519551953\n",
      "355\n",
      "90.44689212334897\n",
      "356\n",
      "90.28610172930945\n",
      "357\n",
      "90.12716931216622\n",
      "358\n",
      "89.9700801605358\n",
      "359\n",
      "89.81481955548016\n",
      "360\n",
      "89.66137277306572\n",
      "361\n",
      "89.50972508688452\n",
      "362\n",
      "89.35986177053714\n",
      "363\n",
      "89.21176810007981\n",
      "364\n",
      "89.06542935643189\n",
      "365\n",
      "88.92083082774764\n",
      "366\n",
      "88.77795781174906\n",
      "367\n",
      "88.63679561802189\n",
      "368\n",
      "88.4973295702737\n",
      "369\n",
      "88.35954500855428\n",
      "370\n",
      "88.22342729143865\n",
      "371\n",
      "88.08896179817239\n",
      "372\n",
      "87.95613393077996\n",
      "373\n",
      "87.82492911613413\n",
      "374\n",
      "87.69533280799018\n",
      "375\n",
      "87.56733048898076\n",
      "376\n",
      "87.44090767257497\n",
      "377\n",
      "87.31604990499932\n",
      "378\n",
      "87.19274276712261\n",
      "379\n",
      "87.07097187630247\n",
      "380\n",
      "86.95072288819696\n",
      "381\n",
      "86.83198149853779\n",
      "382\n",
      "86.71473344486814\n",
      "383\n",
      "86.59896450824353\n",
      "384\n",
      "86.48466051489663\n",
      "385\n",
      "86.37180733786596\n",
      "386\n",
      "86.2603908985888\n",
      "387\n",
      "86.15039716845811\n",
      "388\n",
      "86.0418121703443\n",
      "389\n",
      "85.93462198008166\n",
      "390\n",
      "85.82881272791869\n",
      "391\n",
      "85.72437059993517\n",
      "392\n",
      "85.62128183942258\n",
      "393\n",
      "85.51953274823255\n",
      "394\n",
      "85.41910968808787\n",
      "395\n",
      "85.31999908186232\n",
      "396\n",
      "85.22218741482463\n",
      "397\n",
      "85.1256612358507\n",
      "398\n",
      "85.03040715860065\n",
      "399\n",
      "84.93641186266441\n",
      "400\n",
      "84.8436620946736\n",
      "401\n",
      "84.7521446693815\n",
      "402\n",
      "84.66184647070963\n",
      "403\n",
      "84.57275445276436\n",
      "404\n",
      "84.48485564081982\n",
      "405\n",
      "84.39813713227039\n",
      "406\n",
      "84.31258609755228\n",
      "407\n",
      "84.22818978103366\n",
      "408\n",
      "84.14493550187518\n",
      "409\n",
      "84.06281065485847\n",
      "410\n",
      "83.98180271118719\n",
      "411\n",
      "83.90189921925716\n",
      "412\n",
      "83.8230878053965\n",
      "413\n",
      "83.74535617457826\n",
      "414\n",
      "83.66869211110371\n",
      "415\n",
      "83.59308347925725\n",
      "416\n",
      "83.51851822393357\n",
      "417\n",
      "83.44498437123713\n",
      "418\n",
      "83.37247002905363\n",
      "419\n",
      "83.30096338759573\n",
      "420\n",
      "83.23045271992103\n",
      "421\n",
      "83.16092638242425\n",
      "422\n",
      "83.09237281530346\n",
      "423\n",
      "83.02478054299952\n",
      "424\n",
      "82.95813817461249\n",
      "425\n",
      "82.8924344042907\n",
      "426\n",
      "82.82765801159638\n",
      "427\n",
      "82.76379786184738\n",
      "428\n",
      "82.70084290643375\n",
      "429\n",
      "82.63878218311174\n",
      "430\n",
      "82.57760481627405\n",
      "431\n",
      "82.51730001719704\n",
      "432\n",
      "82.45785708426637\n",
      "433\n",
      "82.3992654031793\n",
      "434\n",
      "82.34151444712565\n",
      "435\n",
      "82.28459377694705\n",
      "436\n",
      "82.22849304127524\n",
      "437\n",
      "82.17320197664995\n",
      "438\n",
      "82.11871040761523\n",
      "439\n",
      "82.0650082467967\n",
      "440\n",
      "82.0120854949587\n",
      "441\n",
      "81.95993224104137\n",
      "442\n",
      "81.9085386621807\n",
      "443\n",
      "81.85789502370663\n",
      "444\n",
      "81.80799167912605\n",
      "445\n",
      "81.75881907008565\n",
      "446\n",
      "81.7103677263177\n",
      "447\n",
      "81.66262826556876\n",
      "448\n",
      "81.61559139351115\n",
      "449\n",
      "81.5692479036378\n",
      "450\n",
      "81.52358867714015\n",
      "451\n",
      "81.47860468277152\n",
      "452\n",
      "81.43428697669316\n",
      "453\n",
      "81.39062670230622\n",
      "454\n",
      "81.34761509006813\n",
      "455\n",
      "81.30524345729421\n",
      "456\n",
      "81.26350320794508\n",
      "457\n",
      "81.22238583240093\n",
      "458\n",
      "81.18188290722019\n",
      "459\n",
      "81.14198609488662\n",
      "460\n",
      "81.10268714354237\n",
      "461\n",
      "81.0639778867091\n",
      "462\n",
      "81.02585024299503\n",
      "463\n",
      "80.9882962157923\n",
      "464\n",
      "80.95130789296039\n",
      "465\n",
      "80.91487744649918\n",
      "466\n",
      "80.87899713221071\n",
      "467\n",
      "80.84365928934936\n",
      "468\n",
      "80.80885634026255\n",
      "469\n",
      "80.77458079001995\n",
      "470\n",
      "80.74082522603354\n",
      "471\n",
      "80.70758231766817\n",
      "472\n",
      "80.67484481584106\n",
      "473\n",
      "80.64260555261477\n",
      "474\n",
      "80.61085744077833\n",
      "475\n",
      "80.5795934734227\n",
      "476\n",
      "80.54880672350558\n",
      "477\n",
      "80.51849034340981\n",
      "478\n",
      "80.48863756449258\n",
      "479\n",
      "80.45924169662894\n",
      "480\n",
      "80.4302961277463\n",
      "481\n",
      "80.40179432335289\n",
      "482\n",
      "80.3737298260595\n",
      "483\n",
      "80.34609625509448\n",
      "484\n",
      "80.31888730581265\n",
      "485\n",
      "80.29209674919846\n",
      "486\n",
      "80.26571843136264\n",
      "487\n",
      "80.23974627303497\n",
      "488\n",
      "80.21417426904992\n",
      "489\n",
      "80.18899648782829\n",
      "490\n",
      "80.1642070708548\n",
      "491\n",
      "80.13980023214901\n",
      "492\n",
      "80.11577025773445\n",
      "493\n",
      "80.09211150510157\n",
      "494\n",
      "80.06881840266831\n",
      "495\n",
      "80.04588544923584\n",
      "496\n",
      "80.02330721344153\n",
      "497\n",
      "80.00107833320875\n",
      "498\n",
      "79.97919351519349\n",
      "499\n",
      "79.95764753422725\n",
      "500\n",
      "79.9364352327595\n",
      "501\n",
      "79.91555152029493\n",
      "502\n",
      "79.89499137283022\n",
      "503\n",
      "79.87474983228927\n",
      "504\n",
      "79.8548220059545\n",
      "505\n",
      "79.83520306589774\n",
      "506\n",
      "79.81588824840985\n",
      "507\n",
      "79.79687285342855\n",
      "508\n",
      "79.77815224396493\n",
      "509\n",
      "79.75972184552886\n",
      "510\n",
      "79.74157714555459\n",
      "511\n",
      "79.72371369282432\n",
      "512\n",
      "79.70612709689189\n",
      "513\n",
      "79.68881302750638\n",
      "514\n",
      "79.67176721403497\n",
      "515\n",
      "79.6549854448858\n",
      "516\n",
      "79.63846356693168\n",
      "517\n",
      "79.6221974849327\n",
      "518\n",
      "79.60618316096013\n",
      "519\n",
      "79.59041661382095\n",
      "520\n",
      "79.5748939184823\n",
      "521\n",
      "79.5596112054971\n",
      "522\n",
      "79.5445646604309\n",
      "523\n",
      "79.52975052328884\n",
      "524\n",
      "79.51516508794482\n",
      "525\n",
      "79.50080470157103\n",
      "526\n",
      "79.48666576406931\n",
      "527\n",
      "79.4727447275039\n",
      "528\n",
      "79.45903809553619\n",
      "529\n",
      "79.44554242286002\n",
      "530\n",
      "79.4322543146401\n",
      "531\n",
      "79.41917042595163\n",
      "532\n",
      "79.40628746122198\n",
      "533\n",
      "79.39360217367498\n",
      "534\n",
      "79.38111136477625\n",
      "535\n",
      "79.36881188368213\n",
      "536\n",
      "79.35670062669016\n",
      "537\n",
      "79.34477453669255\n",
      "538\n",
      "79.33303060263114\n",
      "539\n",
      "79.3214658589569\n",
      "540\n",
      "79.31007738508994\n",
      "541\n",
      "79.29886230488367\n",
      "542\n",
      "79.28781778609185\n",
      "543\n",
      "79.27694103983717\n",
      "544\n",
      "79.26622932008455\n",
      "545\n",
      "79.25567992311689\n",
      "546\n",
      "79.24529018701288\n",
      "547\n",
      "79.23505749112965\n",
      "548\n",
      "79.22497925558733\n",
      "549\n",
      "79.21505294075794\n",
      "550\n",
      "79.20527604675628\n",
      "551\n",
      "79.19564611293575\n",
      "552\n",
      "79.18616071738656\n",
      "553\n",
      "79.17681747643766\n",
      "554\n",
      "79.16761404416239\n",
      "555\n",
      "79.15854811188731\n",
      "556\n",
      "79.14961740770524\n",
      "557\n",
      "79.14081969599137\n",
      "558\n",
      "79.13215277692342\n",
      "559\n",
      "79.12361448600487\n",
      "560\n",
      "79.11520269359376\n",
      "561\n",
      "79.10691530443212\n",
      "562\n",
      "79.09875025718264\n",
      "563\n",
      "79.09070552396689\n",
      "564\n",
      "79.08277910990826\n",
      "565\n",
      "79.07496905267851\n",
      "566\n",
      "79.06727342204876\n",
      "567\n",
      "79.05969031944352\n",
      "568\n",
      "79.05221787749961\n",
      "569\n",
      "79.04485425962856\n",
      "570\n",
      "79.03759765958291\n",
      "571\n",
      "79.03044630102663\n",
      "572\n",
      "79.02339843710972\n",
      "573\n",
      "79.0164523500471\n",
      "574\n",
      "79.00960635070011\n",
      "575\n",
      "79.00285877816405\n",
      "576\n"
     ]
    }
   ],
   "source": [
    "# Train your perceptron to get a model\n",
    "# w_init = np.zeros((len(X[0])))\n",
    "# alpha = 10e-7\n",
    "# max_iter = 1000\n",
    "# print(w)\n",
    "# # Visualize the evolution of the loss on the training set. \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_loss(w, X, Y):\n",
    "#     list = []\n",
    "#     iter = 0\n",
    "#     while iter < max_iter:\n",
    "#         print(iter)\n",
    "#         iter += 1\n",
    "#         w = descent(w_init, X, Y, alpha , iter)\n",
    "#         list.append(emp_loss(w, X, Y))\n",
    "#     plt.plot(list)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_loss(w, X, Y)\n",
    "\n",
    "# Has it converged ?\n",
    "\n",
    "# > Yes, it has converged. Because the loss function converges to a minimum value. \n",
    "\n",
    "# Training for several choices of alpha and max_iter.\n",
    "\n",
    "# What seem like good values for alpha and max_iter ?\n",
    "\n",
    "# > The values of alpha and max_iter that seem good are 10e-5 and 1000 respectively.\n",
    "\n",
    "# What is the loss associated with the final model ?\n",
    "\n",
    "# > The loss associated with the final model is 0.0\n",
    "\n",
    "# Is the final model the optimal one for a perceptron ?\n",
    "\n",
    "# > Yes, the final model is the optimal one for a perceptron.\n",
    "\n",
    "# Train your perceptron to get a model\n",
    "w = np.zeros((len(X[0])))\n",
    "alpha = 0.01\n",
    "max_iter = 1000\n",
    "# Visualize the evolution of the loss on the training set. \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(w, X, Y):\n",
    "    list = []\n",
    "    iter = 0\n",
    "    while iter < max_iter:\n",
    "        print(iter)\n",
    "        iter += 1\n",
    "        w = descent(w, X, Y, alpha , iter)\n",
    "        print(emp_loss(w, X, Y))\n",
    "        list.append(emp_loss(w, X, Y))\n",
    "    plt.plot(list)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(w, X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code sample that can be used to visualize the difference between the ground truth and the prediction\n",
    "\n",
    "num_samples_to_plot = 20\n",
    "plt.plot(Y[0:num_samples_to_plot], 'ro', label='y')\n",
    "yw = [h(w,x) for x in X]\n",
    "plt.plot(yw[0:num_samples_to_plot], 'bx', label='$\\hat{y}$')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Examples\")\n",
    "plt.ylabel(\"f(examples)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Going further\n",
    "\n",
    "The following are extensions of the work previously done. If attempting them **do not modify** the code you produced above so that it can be evaluated.\n",
    "\n",
    "### Improvements to gradient descent\n",
    "\n",
    "Consider improving the gradient descent with:\n",
    "\n",
    " - Stochastic Gradient Descent (SGD), which means selecting a subset of the examples for training\n",
    " - Detection of convergence to halt the algorithm before the maximum number of iterations\n",
    "\n",
    "\n",
    "### Data normalization\n",
    "\n",
    "Different input features can have different units, and very different ranges.\n",
    "Within the perceptron computation, these values will be summed together.\n",
    "While gradient descent is normally able to deal with this (by adapting the weights of the perceptron for each input feature), standardizing the input features usually eases the perceptron training, and can sometimes improve accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(copy=True) \n",
    "X_normalized = sc.fit_transform(X)\n",
    "```\n",
    "\n",
    "Try applying a standard normalization to the input features (make sure that you keep a feature column that is always equal to 1). Is the convergence faster ? Try to quantify this speed-up. What about accuracy ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
