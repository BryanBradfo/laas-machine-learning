{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "475a964d",
   "metadata": {},
   "source": [
    "# Lab: Classification with Decision Trees\n",
    "\n",
    "**Objectives of the practical work:**\n",
    "\n",
    "1. Learn how to build decision trees with scikit-learn Â \n",
    "2. Be familiar with some parameters and visualization tools\n",
    "3. Use a real-case dataset (COMPASS ) as an example\n",
    "4. Evaluate diverse trees in terms of training and testing accuracies with different parameters\n",
    "5. Study the impact of some parameters on the sensitivity aspect\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac3db85d",
   "metadata": {},
   "source": [
    "## PART 1: Basic steps \n",
    "\n",
    "The following are basic instructions to start with decision trees. You need to execute them one by one to understand the basic steps for learning decision trees. Once you get familiar with the different steps, you will be working on the compass dataset. \n",
    "\n",
    "The decision tree package that we use is from scikit-learn. The full documentation of decision trees are available at https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "Take a moment to briefly consult the documentation.\n",
    "\n",
    "We need first to include some libraries: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11530ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt # for a good visualization of the trees "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56ce240f",
   "metadata": {},
   "source": [
    "The following is a basic example for binary classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169f6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the training set \n",
    "# Each example in X has 4 binary features\n",
    "X = [[0, 0, 1, 0], [0, 1, 0, 1] , [1, 1, 0, 0] , [1, 0, 1, 1] , [0, 0, 0, 1] , [1, 1, 1, 0]]\n",
    "\n",
    "# Y is the classes associated with the training set. \n",
    "# For instance the label of the first and second example is 1; of the third example is 0, etc\n",
    "Y = [1, 1, 0, 0, 1, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5f396bd",
   "metadata": {},
   "source": [
    "We construct a decision tree using the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a75d5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adfe42ed",
   "metadata": {},
   "source": [
    "Now we can ask the decision tree to predict the outcome for unknown examples. \n",
    "For instance we can ask a prediction for the three examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de16e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[1,1,1,1] , [0,1,0,0] , [1,1,0,1] ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "466faea1",
   "metadata": {},
   "source": [
    "The result is an array of the 3 predicted labels (one for each example): `array([0, 1, 0])`\n",
    "\n",
    "## PART 2 : Visualization \n",
    "\n",
    "There are many ways to visualize a decision tree. The first one is very basic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67433f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature_0 <= 0.50\n",
      "|   |--- class: 1\n",
      "|--- feature_0 >  0.50\n",
      "|   |--- feature_1 <= 0.50\n",
      "|   |   |--- class: 0\n",
      "|   |--- feature_1 >  0.50\n",
      "|   |   |--- feature_2 <= 0.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |   |--- feature_2 >  0.50\n",
      "|   |   |   |--- class: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_representation = tree.export_text(clf)\n",
    "print(text_representation)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65508fca",
   "metadata": {},
   "source": [
    "We can use a more readable and visual way as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94629400",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (2516191881.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    feature_names= (\"f1\",\"f2\" , \"f3\", \"f4\"),\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "_ = tree.plot_tree(clf, \n",
    "                    feature_names= (\"f1\",\"f2\" , \"f3\", \"f4\"),\n",
    "                    class_names= (\"false (0)\", \"true (1)\" ), \n",
    "                    filled=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3d0a912",
   "metadata": {},
   "source": [
    "Where:\n",
    "- `figsize` restrains the size of the plot,\n",
    "- `feature_names` gives the names of the different features,\n",
    "- `class_names` corresponds to human readable labels for each class,\n",
    "- `filled` is a boolean indicating a preference to show a colorful tree. \n",
    "\n",
    "\n",
    "**Tasks:**\n",
    "- Construct manually a new binary dataset (larger than the one above), associate some labels then study the tree built by default (similar to above). Give some fancy names to the binary features and classes for a visual interpretation. \n",
    "\n",
    "\n",
    "\n",
    "## PART 3: The compass dataset\n",
    "\n",
    "We study here the COMPASS dataset as a case study. Recall that it has been used in a legislative context for predicting recidivism in the U.S. That is, the tendency of a convicted criminal to re-offend\n",
    "\n",
    "\n",
    "Have a look at the original non-binary dataset ([https://www.kaggle.com/danofer/compass](https://www.kaggle.com/danofer/compass)) to understand the different features. Consider in particular the data used for fairness: propublicaCompassRecividism_data_fairml.csv\n",
    "\n",
    "\n",
    "**Understanding the dataset:**\n",
    "\n",
    "Take a moment to think about the following questions \n",
    "\n",
    "- What are the features? \n",
    "- How many examples in the dataset?\n",
    "- What are your expectations regarding the most important features? \n",
    "- Propose (informally) a way to reduce the dataset\n",
    "- There many ways to binarize the dataset. How do you propose to do so?\n",
    "\n",
    "\n",
    "\n",
    "Below, we use a binarized version of the dataset that is used in the FairCORELS library (https://github.com/ferryjul/fairCORELS) as well some of its tools. \n",
    "\n",
    "You need first to download the dataset and the tools file and put them in your work directory:\n",
    "\n",
    "- The dataset compass.csv\n",
    "- The set of tools utils.py\n",
    "\n",
    "Load the binary dataset `compass.csv` as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from utils import load_from_csv\n",
    "\n",
    "train_examples, train_labels, features, prediction = load_from_csv(\"./compass.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cb13fc2",
   "metadata": {},
   "source": [
    "Inspect each of these 4 objects. What do they represent? How many features? examples? \n",
    "\n",
    "Have a look at the different parameters of the `DecisionTreeClassifier` class constructor. We will be studying three parameters: \n",
    "\n",
    "- splitter\n",
    "- max_depth \n",
    "- min_samples_leaf \n",
    "\n",
    "What do they represent? \n",
    "\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "- 1:  Build severals decision trees (different parameters) and visualize them\n",
    "- 2: Run a solid evaluation on different trees (with different parameters) by randomly splitting the data 80% for training and 20% for test *multiple times*.\n",
    "- 3: Do again the evaluation using 5-cross-validation\n",
    "- 4: Evaluate the impact (in terms of accuracy) of the three parameters : maximum depth, splitting criterion, and the minimum sample leafs. \n",
    "- 5: Study the confusion matrix to evaluate the True/False Positive/Negative Rate. What are the most important parameters? \n",
    "- 6: Propose a way to assess whether the algorithm is fair to a particular ethnic group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd3275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
